python finetune.py --base_model 'mosaicml/mpt-7b-instruct' --data_path 'yahma/alpaca-cleaned' --output_dir './lora-mpt2' --lora_target_modules '[Wqkv]' --lora_r 8 --cutoff_len 768 --batch_size 256 --micro_batch_size 16

python generate.py --load_8bit --base_model 'mosaicml/mpt-7b-instruct' --lora_weights 'lora-mpt'