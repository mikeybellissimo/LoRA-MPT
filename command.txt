python finetune.py --base_model 'mosaicml/mpt-7b-instruct' --data_path 'yahma/alpaca-cleaned' --output_dir './lora-mpt2' --lora_target_modules '[Wqkv]' --lora_r 4 --cutoff_len 512 --batch_size 128 --micro_batch_size 4

python generate.py --load_8bit --base_model 'mosaicml/mpt-7b-instruct' --lora_weights 'lora-mpt'

pip install flash-attn==1.0.3.post0 triton==2.0.0.dev20221202 